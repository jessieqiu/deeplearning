{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(1020, 64, 64, 3) uint8\n",
      "(1020,) uint8\n"
     ]
    }
   ],
   "source": [
    "train_x = np.load('ex5_train_x.npy')\n",
    "train_y_original = np.load('ex5_train_y.npy')\n",
    "train_y = train_y_original.astype('uint8')\n",
    "#train_x\n",
    "print(train_y.max())\n",
    "print(train_x.shape, train_x.dtype)\n",
    "print(train_y.shape, train_y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split data\n",
    "def split(x,y,test_portion):\n",
    "    #try shuffle here?\n",
    "    size = x.shape[0]    \n",
    "    train_size = int(size * (1-test_portion))\n",
    "    x_train = x[0:train_size,:,:,:]\n",
    "    y_train = y[0:train_size]\n",
    "    print(x_train.shape, y_train.shape)\n",
    "\n",
    "    test_size = size - train_size\n",
    "    x_test = x[train_size:,:,:,:]\n",
    "    y_test = y[train_size:]\n",
    "\n",
    "    print(x_test.shape, y_test.shape)\n",
    "\n",
    "    return train_size, x_train, y_train, test_size, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(y):\n",
    "    length = y.shape[0]\n",
    "    #print(length)\n",
    "    output = np.zeros((6,length))      \n",
    "    for i in range(length):         \n",
    "        if(y[i][0]>5):  #to prevent index out of bound\n",
    "            print('label is wrong');\n",
    "            return -1\n",
    "        else:\n",
    "            #print(y[i])\n",
    "            output[y[i][0]][i]=1   \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(816, 64, 64, 3) (816,)\n",
      "(204, 64, 64, 3) (204,)\n",
      "(816, 6)\n",
      "(204, 6)\n"
     ]
    }
   ],
   "source": [
    "train_size, x_train, y_train, test_size, x_test, y_test = split(train_x,train_y,0.2)\n",
    "\n",
    "#one_col = y_train.reshape(y_train.shape[0],1)\n",
    "#y_train_one_hot = one_hot_encoding(one_col)\n",
    "\n",
    "X_train = x_train/255  # do i need mean center here??\n",
    "X_test = x_test/255\n",
    "Y_train = one_hot_encoding(y_train.reshape(y_train.shape[0],1)).T\n",
    "Y_test = one_hot_encoding(y_test.reshape(y_test.shape[0],1)).T\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation_forward(z, activation_index=1):      \n",
    "    activation_set = [\"sigmoid\",\"ReLU\"]    \n",
    "    if(activation_index >= len(activation_set)):\n",
    "        print('no such function')\n",
    "        return -1       \n",
    "    #sigmoid\n",
    "    if activation_index == 0:  \n",
    "        #check z shape?\n",
    "        #print(z.shape)\n",
    "        a =  1.0/(1.0 + np.exp(-z))\n",
    "        #a = np.divide(1, np.add (1 , np.exp(-z)))        \n",
    "    #ReLU\n",
    "    else:\n",
    "        a = np.maximum(z, np.zeros(z.shape)) #will take max(z[i],0) \n",
    "    return a\n",
    "\n",
    "\n",
    "def activation_backward(da, a,z,function_index=1,alp=1):\n",
    "    activation_set = [\"sigmoid\",\"ReLU\",\"tanh\",\"Leaky ReLU\",\"ELU\"]\n",
    "    \n",
    "    if(function_index >= len(activation_set)):\n",
    "        print('no such function')\n",
    "        return -1\n",
    "    \n",
    "    if function_index== 0:\n",
    "        dadz = a * (1 - a)\n",
    "    elif function_index == 1:\n",
    "        dadz = np.ones(z.shape)        \n",
    "        dadz[z < 0] = 0 \n",
    "    elif function_index == 2:\n",
    "        dadz = 1 - a*a    \n",
    "    elif function_index==3:\n",
    "        dadz = np.ones(z.shape)\n",
    "        dadz[z < 0] = alp \n",
    "    else:\n",
    "        dadz = np.ones(z.shape)\n",
    "        dadz[z < 0] = alp * np.exp(dadz[z < 0]) \n",
    "    if(da.shape!=dadz.shape):\n",
    "        print(\"shape not match in activation backward\")\n",
    "        dz = -1\n",
    "    else:\n",
    "        dz = np.multiply(da,dadz)\n",
    "        \n",
    "    return dz        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test and plots for activation functions\n",
    "# z1 = np.arange(-5,5,0.1)\n",
    "\n",
    "# a1 = activation_forward(z1,0)\n",
    "# plt.plot(z1, a1)\n",
    "# plt.title('sigmoid')\n",
    "# plt.show()\n",
    "\n",
    "# a2 = activation_forward(z1,1)\n",
    "# plt.plot(z1, a2)\n",
    "# plt.title('ReLU')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_cross_entropy_loss(a, i=0, j=1020):\n",
    "    # Y_train is global variable\n",
    "    if(a.shape==Y_train[i:j,:].shape):         \n",
    "        if(a.any()==0):\n",
    "            #print ('shift to right')\n",
    "            a[a==0] = 0.00000001            \n",
    "        if(a.any()==1):\n",
    "            #print ('shift to left') \n",
    "            a[a==1] = 0.99999999                      \n",
    "        temp1 = Y_train[i:j,:] * np.log(a)\n",
    "        temp2 = (1 - Y_train[i:j,:]) * np.log(1 - a)  #could be wrong here in cost / loss ?\n",
    "        #print (\"total cost %f\" %np.sum(temp1 + temp2))\n",
    "        #print (\"i j\", i, j, \"batch size\", j-i)\n",
    "        loss = (-1 / (j-i)) * np.sum(temp1 + temp2)  #j must > i         \n",
    "    else:\n",
    "        print('I did something wrong, shape of a is ', a.shape, ' it should be ', Y_train[i:j,:].shape)\n",
    "        loss = -1\n",
    "    return loss\n",
    "\n",
    "def accuracy(prediction,label):\n",
    "    if (prediction.shape!=label.shape):\n",
    "        print ('wrong input size')\n",
    "        print (prediction.shape)\n",
    "        print (label.shape)        \n",
    "        return -1\n",
    "    count = 0\n",
    "    total = prediction.shape[0]\n",
    "    for i in range(total):        \n",
    "        if prediction[i]!= label[i]:\n",
    "            count+=1\n",
    "    #print 'error count', count\n",
    "    result = 0.0\n",
    "    error_rate = float((count)/total)\n",
    "    accuracy = 1-error_rate\n",
    "    print (\"accuracy\", accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "def save_model(para, path):\n",
    "    f = h5py.File(path,'w')\n",
    "    for k, v in para.items():\n",
    "        f.create_dataset(k, data=v)\n",
    "    f.close()\n",
    "    print(\"file saved\")\n",
    "\n",
    "def load_model(path):\n",
    "    dataset = h5py.File(path,'r')\n",
    "    dataset.keys()\n",
    "    para = {}\n",
    "    for i in dataset.keys():\n",
    "        para[i] = np.array(dataset[i])\n",
    "        #print(para[i].shape)\n",
    "    dataset.close()\n",
    "    return para\n",
    "\n",
    "def print_model(parameters):\n",
    "    print(\"relu\",\"max\",\"relu\",\"max\",\"sigmoid\",\"sigmoid\")\n",
    "    for k, v in parameters.items():\n",
    "        print (k,v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_pad (x,n):\n",
    "    #x_padded = np.pad(x, (n,), 'constant') #could be wrong here\n",
    "    x_padded = np.pad(x, ((0, 0), (n, n), (n, n), (0, 0)), 'constant')\n",
    "    return x_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_convolution_step(a,kernel,b):\n",
    "    if(a.shape!=kernel.shape):\n",
    "        print (\"shape do not match:\")\n",
    "        print(a.shape)\n",
    "        print(kernel.shape)\n",
    "        return -1\n",
    "    else:\n",
    "        #print (\"shape match:\")\n",
    "        #print(a.shape, kernel.shape)\n",
    "        #print (\"conv sum\", np.sum(np.multiply(a,kernel)) + b  )\n",
    "        return np.sum(np.multiply(a,kernel)) + b   \n",
    " \n",
    "## test\n",
    "# a=np.arange(1,7).reshape(2,3)\n",
    "# b=np.arange(3,9).reshape(2,3)\n",
    "# print(a,\"\\n\",b)\n",
    "# print(single_convolution_step(a,b,2))\n",
    "\n",
    "# d3 = np.ones((3,2,2))\n",
    "# print(d3)\n",
    "# np.sum(d3, axis=0)\n",
    "\n",
    "def convolution_forward_one_layer(x,kernel,bias,stride=1,pad=2):\n",
    "    batch_size = x.shape[0]  \n",
    "    x_h = x.shape[1]\n",
    "    x_w = x.shape[2]\n",
    "    x_c = x.shape[3]   \n",
    "    f_h = kernel.shape[0] #switch here??\n",
    "    f_w = kernel.shape[1] #switch here??\n",
    "    #print(f_w,f_h)\n",
    "    channel = kernel.shape[3]    \n",
    "    w = int((x_w-f_w+pad*2)/stride) + 1\n",
    "    h = int((x_h-f_h+pad*2)/stride) + 1  \n",
    "    \n",
    "    x_pads = zero_pad(x, pad)\n",
    "    #x_pads = zero_pad(A_prev, pad)\n",
    "    output_4d = np.zeros((batch_size,h,w,channel))    \n",
    "    #print(output_4d.shape)\n",
    "    output_3d = np.zeros((batch_size,w,h))\n",
    "    \n",
    "    for batch in range(batch_size):\n",
    "        x_pad = x_pads[batch]\n",
    "#         print(\"x_pad shape\", x_pad.shape)\n",
    "#         print (kernel.shape)\n",
    "        for j in range(h):\n",
    "            for i in range(w):\n",
    "                for c in range(channel):              \n",
    "                    #print(\"conv forward: \", batch, j, i, c)\n",
    "                    #print(\"h begin, h end, w begin, w end:\",j*stride, j*stride+f_h, i*stride, i*stride+f_w)\n",
    "                    output_4d[batch][j][i][c]= single_convolution_step(x_pad[j*stride:j*stride+f_h, i*stride:i*stride+f_w,:], kernel[...,c], bias[...,c])\n",
    "        #output_3d[batch] = np.sum(output_4d[batch], axis=0)\n",
    "         \n",
    "    assert(output_4d.shape == (batch_size, h,w, channel))\n",
    "    \n",
    "    cache = (x, kernel, bias, channel, stride, pad)\n",
    "\n",
    "    return output_4d, cache\n",
    "\n",
    "    #return output_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pooling_forward_one_layer (x,stride=1,f_w=2,f_h=2,function = \"max\"):\n",
    "\n",
    "    batch_size, x_h, x_w, x_c = x.shape\n",
    "    #print(batch_size, x_h, x_w, x_c)\n",
    "    \n",
    "    w = int((x_w-f_w)/stride) + 1\n",
    "    h = int((x_h-f_h)/stride) + 1  \n",
    "    channel = x_c\n",
    "    #print(w,h,channel)\n",
    "    \n",
    "    output_4d = np.zeros((batch_size, h,w,channel))\n",
    "           \n",
    "    for batch in range(batch_size):\n",
    "        for j in range(h):\n",
    "            for i in range(w):\n",
    "                for c in range(channel):              \n",
    "                    x_slice = x[batch, j*stride:j*stride+f_h, i*stride:i*stride+f_w,c]\n",
    "                    if(function ==\"max\"):\n",
    "                        output_4d[batch][j][i][c]= np.max(x_slice)\n",
    "                    elif(function == \"ave\"):\n",
    "                        output_4d[batch][j][i][c]= np.mean(x_slice)\n",
    "      \n",
    "    assert(output_4d.shape == (batch_size, h,w, channel))\n",
    "    \n",
    "    cache = (x, channel, stride, f_w, f_h )\n",
    "\n",
    "    return output_4d, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_mask(x):\n",
    "    \n",
    "    return (x==np.max(x))\n",
    "\n",
    "# np.random.seed(1)\n",
    "# x = np.random.randn(3,2)\n",
    "# mask = max_mask(x)\n",
    "# print('x = ', x)\n",
    "# print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribute (dp, h, w):\n",
    "     \n",
    "    return np.ones((h,w)) * dp / float(h*w)\n",
    "\n",
    "# a = distribute(2, 2, 2)\n",
    "# print('distributed value =', a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pooling_backward_one_layer (da, cache, function = \"max\"):\n",
    "    \n",
    "    #cache = (x, channel, stride, f_w, f_h )\n",
    "    x_prev, channel, stride, f_w, f_h = cache    \n",
    "    batch_size, x_h_prev, x_w_prev, x_c_prev = x_prev.shape\n",
    "    _,h,w,channel = da.shape    \n",
    "    da_prev = np.zeros(x_prev.shape)\n",
    "    \n",
    "    for batch in range(batch_size):\n",
    "        x_p = x_prev[batch]\n",
    "        for j in range(h):\n",
    "            for i in range(w):\n",
    "                for c in range(channel):                       \n",
    "                    \n",
    "                    if(function ==\"max\"):\n",
    "                        #output_4d[batch][j][i][c]= np.max(x_slice)\n",
    "                        x_p_slice = x_p[j*stride:j*stride+f_h, i*stride:i*stride+f_w,c]\n",
    "                        mask = max_mask(x_p_slice)\n",
    "                        da_prev[batch, j*stride:j*stride+f_h, i*stride:i*stride+f_w,c] += np.multiply(mask, da[batch][j][i][c])\n",
    "                        \n",
    "                    elif(function == \"ave\"):\n",
    "                        #output_4d[batch][j][i][c]= np.mean(x_slice)                        \n",
    "                        da_p = da[batch][j][i][c]\n",
    "                        da_prev[batch, j*stride:j*stride+f_h, i*stride:i*stride+f_w,c] += distribute(da_p, f_h, f_w)\n",
    "   \n",
    "    assert(da_prev.shape == x_prev.shape)\n",
    "    return da_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution_backward_one_layer(dz,cache):\n",
    "         \n",
    "    x_prev, kernel, bias, channel, stride, pad = cache  \n",
    "    (batch_size, x_h_prev, x_w_prev, x_c_prev) = x_prev.shape\n",
    "    (f_h, f_w, channel_prev, channel) = kernel.shape  \n",
    "    _, x_h, x_w, x_c = dz.shape\n",
    "    \n",
    "    output = np.zeros((batch_size,x_h_prev, x_w_prev, x_c_prev))                           \n",
    "    dw = np.zeros((f_h, f_w, channel_prev, channel))\n",
    "    db = np.zeros((1, 1, 1, channel))\n",
    "    x_prev_pad = zero_pad(x_prev, pad)\n",
    "    dx_prev_pad = zero_pad(output, pad)\n",
    "    \n",
    "    for batch in range(batch_size):      \n",
    "        a_prev_pad = x_prev_pad[batch]\n",
    "        da_prev_pad = dx_prev_pad[batch]    \n",
    "        for j in range(x_h):\n",
    "            for i in range(x_w):\n",
    "                for c in range(x_c):  \n",
    "                    a_slice = a_prev_pad[j*stride:j*stride+f_h, i*stride:i*stride+f_w,:]                     \n",
    "                    da_prev_pad[j*stride:j*stride+f_h, i*stride:i*stride+f_w,:] += kernel[...,c] * dz[batch][j][i][c]\n",
    "                   \n",
    "                    dw[:,:,:,c] += a_slice * dz[batch][j][i][c]\n",
    "                    db[:,:,:,c] += dz[batch][j][i][c]                    \n",
    "       \n",
    "        output[batch, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]   \n",
    "    \n",
    "    assert(output.shape == (batch_size, x_h_prev, x_w_prev, x_c_prev))\n",
    "    \n",
    "    return output, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialization(function=1):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    #s = np.sum(shape)\n",
    "    c= 0.0  \n",
    "    if (function== 0):\n",
    "        c = math.sqrt(2)\n",
    "    elif (function== 2):\n",
    "        c = 4 * math.sqrt(2)\n",
    "    else:\n",
    "        c = 2    \n",
    "    #print ('sigma,', c)    \n",
    "    \n",
    "    shape = np.zeros((6,4))\n",
    "    shape[1] = (4, 4, 3, 8)\n",
    "    shape[2] = (1, 1, 3, 8)\n",
    "    shape[3] = (2, 2, 8, 16)\n",
    "    shape[4] = (1, 1, 8, 16)\n",
    "    #shape[5] = (16, 6)\n",
    "    #shape[6] = (1, 6)    \n",
    "        \n",
    "    w1 = np.random.normal(0, c / math.sqrt(10000) ,(4, 4, 3, 8))\n",
    "    w2 = np.random.normal(0, c / math.sqrt(10000) ,(4, 4, 8, 16))\n",
    "    \n",
    "    w3 = np.random.normal(0, c / math.sqrt(10000) ,(4096, 6))\n",
    "#     w3 = np.random.normal(0, c / math.sqrt(10000) ,(4096, 256))\n",
    "    w4 = np.random.normal(0, c / math.sqrt(10000) ,(256, 6))\n",
    "    \n",
    "    b1 = np.random.normal(0, c / math.sqrt(10000) ,(1, 1, 1, 8))\n",
    "    b2 = np.random.normal(0, c / math.sqrt(10000) ,(1, 1, 1, 16))\n",
    "    \n",
    "    b3 = np.random.normal(0, c / math.sqrt(10000) ,(1, 6))\n",
    "#     b3 = np.random.normal(0, c / math.sqrt(10000) ,(1, 256))\n",
    "    b4 = np.random.normal(0, c / math.sqrt(10000) ,(1, 6))\n",
    "       \n",
    "    parameters = {\"w1\": w1,\n",
    "                  \"w2\": w2,\n",
    "                  \"w3\": w3,\n",
    "                  \"w4\": w4,\n",
    "                  \"b1\": b1,\n",
    "                  \"b2\": b2,\n",
    "                  \"b3\": b3,\n",
    "                  \"b4\": b4\n",
    "                 }\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['w1', 'w2', 'w3', 'w4', 'b1', 'b2', 'b3', 'b4'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_para = initialization(1)\n",
    "temp_para.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "        \n",
    "    w1 = parameters[\"w1\"]\n",
    "    w2 = parameters[\"w2\"]\n",
    "    w3 = parameters[\"w3\"]\n",
    "    w4 = parameters[\"w4\"]\n",
    "    \n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    b4 = parameters[\"b4\"]\n",
    "            \n",
    "    \n",
    "    #def convolution_forward_one_layer(x,kernel,bias,stride=1,pad=2):\n",
    "    Z1, c_cache1 = convolution_forward_one_layer(X,w1,b1,2,2)\n",
    "    A1 = activation_forward(Z1,1)\n",
    "    P1, p_cache1 = pooling_forward_one_layer (A1,1,2,2,\"max\")   #change to parameters later\n",
    "\n",
    "    Z2, c_cache2 = convolution_forward_one_layer(P1,w2,b2,2,2)\n",
    "    A2 = activation_forward(Z2,1)\n",
    "    P2, p_cache2 = pooling_forward_one_layer (A2,1,2,2,\"max\")\n",
    "    P2_flat = P2.reshape(P2.shape[0],P2.shape[1]*P2.shape[2]*P2.shape[3])\n",
    "    #print(P2.shape, P2_flat.shape)\n",
    "    \n",
    "    Z3 = np.dot(P2_flat,w3) + b3\n",
    "    A3 = activation_forward(Z3,0)  \n",
    "    \n",
    "    Z4 = Z3\n",
    "    A4 = A3\n",
    "    \n",
    "#     Z4 = np.dot(A3, w4) + b4\n",
    "#     A4 = activation_forward(Z4,0) \n",
    "    \n",
    "    # last layer need to be sigmoid !!!\n",
    "    \n",
    "    \n",
    "    all_cache = { \n",
    "                  \"z1\": Z1,\n",
    "                  \"c_cache1\": c_cache1,\n",
    "                  \"a1\": A1,        \n",
    "                  \"p1\": P1,\n",
    "                  \"p_cache1\": p_cache1,\n",
    "        \n",
    "                  \"z2\": Z2,\n",
    "                  \"c_cache2\": c_cache2,\n",
    "                  \"a2\": A2,\n",
    "                  \"p2\": P2,\n",
    "                  \"p_cache2\": p_cache2,            \n",
    "                  \n",
    "                  \"p2_flat\": P2_flat,\n",
    "                  \"z3\": Z3,\n",
    "                  \"a3\": A3,\n",
    "        \n",
    "                  \"z4\": Z4,\n",
    "                  \"a4\": A4\n",
    "                 }\n",
    "    \n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    #P2 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')\n",
    "    # FLATTEN\n",
    "    \n",
    "    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n",
    "    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n",
    "    \n",
    "    #Z3 = tf.contrib.layers.fully_connected(P2, 6, activation_fn=None)\n",
    "\n",
    "    return A4, all_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lastLayerGradient(a, X, i=0, j=1020):  #i j can be used for mini batch, X is the input to last layer\n",
    "    if(a.shape==Y_train[i:j,:].shape):     \n",
    "        dldz = a - Y_train[i:j,:]\n",
    "        m = j-i\n",
    "        dldw = np.dot(np.transpose(X), dldz)/(m) # m or -m?\n",
    "        dldb = np.sum(dldz, axis = 0, keepdims = True)/(m)\n",
    "        #print(\"in lastLayerGradient\",dldw.shape, dldb.shape)\n",
    "        return dldz, dldw, dldb\n",
    "    else:\n",
    "        print('I did something wrong, shape of a is ', a.shape, ' it should be ', Y_train[i:j,:].shape)\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, begin,end, learning_rate = 0.01):\n",
    "        \n",
    "     \n",
    "    #dldz_4, dldw_4, dldb_4 = lastLayerGradient(cache[\"a4\"] , cache[\"a3\"] )    \n",
    "    dldz_3, dldw_3, dldb_3 = lastLayerGradient(cache[\"a3\"] , cache[\"p2_flat\"], begin, end)    #might be wrong here\n",
    "    \n",
    "    # need do backward for fully connected layer\n",
    "    #times weight??? w3\n",
    "    #print(\"dldz_3 shape\",dldz_3.shape)\n",
    "    da = np.dot(parameters[\"w3\"], dldz_3.T) \n",
    "    #print(\"da shape\", da.shape)\n",
    "    \n",
    "    \n",
    "#     print(\"w1 shape\", parameters[\"w1\"].shape)\n",
    "#     print(\"w2 shape\", parameters[\"w2\"].shape)\n",
    "#     print(\"w3 shape\", parameters[\"w3\"].shape)\n",
    "#     print(\"b1 shape\", parameters[\"b1\"].shape)\n",
    "#     print(\"b2 shape\", parameters[\"b2\"].shape)\n",
    "#     print(\"b3 shape\", parameters[\"b3\"].shape)\n",
    "\n",
    " \n",
    "    \n",
    "    #reshape here?\n",
    "    p2_shape = cache[\"p2\"].shape\n",
    "    #print(\"p2_shape\", p2_shape)\n",
    "    dA3 = da.reshape(p2_shape[0],p2_shape[1],p2_shape[2],p2_shape[3])  #could be wrong here\n",
    "    #print(\"dA3 shape\", dA3.shape)\n",
    "    \n",
    "    # activation_backward(da, a,z,function_index=1,alp=1)\n",
    "    dA2_prev = pooling_backward_one_layer(dA3, cache[\"p_cache2\"], \"max\")\n",
    "    dZ2 = activation_backward(dA2_prev,cache[\"a2\"], cache[\"z2\"], 1)\n",
    "    dA2, dw2, db2 = convolution_backward_one_layer(dZ2, cache[\"c_cache2\"])\n",
    "\n",
    "    dA1_prev = pooling_backward_one_layer(dA2, cache[\"p_cache1\"], \"max\")\n",
    "    dZ1 = activation_backward(dA1_prev, cache[\"a1\"], cache[\"z1\"], 1)\n",
    "    dA1, dw1, db1 = convolution_backward_one_layer(dZ1, cache[\"c_cache1\"])\n",
    "\n",
    " \n",
    "    #update the value instead of making a copy here?\n",
    "\n",
    "    #update weights    \n",
    "    parameters[\"w1\"] -= learning_rate*dw1\n",
    "    parameters[\"w2\"] -= learning_rate*dw2\n",
    "    parameters[\"w3\"] -= learning_rate*dldw_3\n",
    "    #parameters[\"w4\"] -= learning_rate*dldw_4\n",
    "\n",
    "    #update bias \n",
    "    parameters[\"b1\"] -= learning_rate*db1\n",
    "    parameters[\"b2\"] -= learning_rate*db2\n",
    "    parameters[\"b3\"] -= learning_rate*dldb_3\n",
    "    #parameters[\"b4\"] -= learning_rate*dldb_4\n",
    "\n",
    "   \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(parameters, learning_rate = 0.01, epochs = 1, mini_batch_size = 50):\n",
    "       \n",
    "    total_size = X_train.shape[0]\n",
    "    iter_per_epoch = math.ceil(total_size/mini_batch_size)\n",
    "    for i in range(epochs):\n",
    "        cost = 0\n",
    "        for j in range(iter_per_epoch):\n",
    "            \n",
    "            begin = j * mini_batch_size            \n",
    "            if(j < iter_per_epoch -1):\n",
    "                end = begin + mini_batch_size\n",
    "            else:\n",
    "                end = total_size\n",
    "            #print(begin, end)\n",
    "            \n",
    "            pred, All_cache = forward_propagation(X_train[begin:end,...], parameters)\n",
    "            \n",
    "            batch_cost = calculate_cross_entropy_loss(pred,begin,end)\n",
    "            print(\"iteration\",j,\"batch_cost\", batch_cost)\n",
    "            cost = cost + batch_cost * (end-begin)\n",
    "            \n",
    "            #def backward_propagation(parameters, cache, learning_rate = 0.01):\n",
    "            parameters = backward_propagation(parameters,All_cache,begin,end,learning_rate)\n",
    "        \n",
    "        \n",
    "        #after one epoch\n",
    "        cost = cost/total_size\n",
    "        print(\"-----> epoch: \", i, \", total cost: \",cost)\n",
    "        \n",
    "    print(\"All done!!! yaaay\")\n",
    "            \n",
    "    return parameters, pred\n",
    "\n",
    "\n",
    "\n",
    "# print(\"prediction \\n\", pred)\n",
    "# print(\"true \\n\",Y_train[begin: end])\n",
    "# cost = calculate_cross_entropy_loss(pred, begin,end)\n",
    "# print(\"cost\",cost)\n",
    "# predict_test = np.argmax(a3_test, axis=0)\n",
    "# test_length = len(predict_test)\n",
    "# test_acc = accuracy(predict_test.reshape(test_length,1),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do not run this, be careful!!!!\n",
    "#parameters = initialization(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the train function\n",
    "#model_para, predic = training(parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_f = \"para_Bijie_Qiu.dat\"\n",
    "# save_model(model_para, path_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu max relu max sigmoid sigmoid\n",
      "b1 (1, 1, 1, 8)\n",
      "b2 (1, 1, 1, 16)\n",
      "b3 (1, 6)\n",
      "b4 (1, 6)\n",
      "w1 (4, 4, 3, 8)\n",
      "w2 (4, 4, 8, 16)\n",
      "w3 (4096, 6)\n",
      "w4 (256, 6)\n",
      "iteration 0 batch_cost 2.70841660531\n",
      "iteration 1 batch_cost 2.72387615011\n",
      "iteration 2 batch_cost 2.70168580797\n",
      "iteration 3 batch_cost 2.72427196487\n",
      "iteration 4 batch_cost 2.71309212488\n",
      "iteration 5 batch_cost 2.72142140315\n",
      "iteration 6 batch_cost 2.71546507172\n",
      "iteration 7 batch_cost 2.72395467484\n",
      "iteration 8 batch_cost 2.656985792\n",
      "-----> epoch:  0 , total cost:  2.71535557961\n",
      "iteration 0 batch_cost 2.70493158253\n",
      "iteration 1 batch_cost 2.72138971349\n",
      "iteration 2 batch_cost 2.69804244767\n",
      "iteration 3 batch_cost 2.72200088315\n",
      "iteration 4 batch_cost 2.71027627449\n",
      "iteration 5 batch_cost 2.71925628926\n",
      "iteration 6 batch_cost 2.71303479087\n",
      "iteration 7 batch_cost 2.72201242171\n",
      "iteration 8 batch_cost 2.65143249115\n",
      "-----> epoch:  1 , total cost:  2.71264382374\n",
      "iteration 0 batch_cost 2.70225973155\n",
      "iteration 1 batch_cost 2.71959610357\n",
      "iteration 2 batch_cost 2.69520137639\n",
      "iteration 3 batch_cost 2.72037618943\n",
      "iteration 4 batch_cost 2.70815157871\n",
      "iteration 5 batch_cost 2.71769713132\n",
      "iteration 6 batch_cost 2.71122950428\n",
      "iteration 7 batch_cost 2.7206372675\n",
      "iteration 8 batch_cost 2.64680535156\n",
      "-----> epoch:  2 , total cost:  2.71061736998\n",
      "iteration 0 batch_cost 2.70020115221\n",
      "iteration 1 batch_cost 2.71831363404\n",
      "iteration 2 batch_cost 2.6929708446\n",
      "iteration 3 batch_cost 2.71922806317\n",
      "iteration 4 batch_cost 2.70654587084\n",
      "iteration 5 batch_cost 2.71658545696\n",
      "iteration 6 batch_cost 2.70989103364\n",
      "iteration 7 batch_cost 2.71968028421\n",
      "iteration 8 batch_cost 2.64291638048\n",
      "-----> epoch:  3 , total cost:  2.70910330399\n",
      "iteration 0 batch_cost 2.69860766838\n",
      "iteration 1 batch_cost 2.71740870647\n",
      "iteration 2 batch_cost 2.69120799848\n",
      "iteration 3 batch_cost 2.71843146455\n",
      "iteration 4 batch_cost 2.70533143928\n",
      "iteration 5 batch_cost 2.71580445075\n",
      "iteration 6 batch_cost 2.7089021812\n",
      "iteration 7 batch_cost 2.71903151742\n",
      "iteration 8 batch_cost 2.63962173745\n",
      "-----> epoch:  4 , total cost:  2.70797364026\n",
      "All done!!! yaaay\n"
     ]
    }
   ],
   "source": [
    "path_f_2 = \"para_Bijie_Qiu2.dat\"\n",
    "#paras = load_model(path_f)\n",
    "paras = load_model(path_f_2)\n",
    "print_model(paras)\n",
    "new_paras, predicts = training(paras,epochs = 5,learning_rate = 0.1,mini_batch_size=100) #learning_rate = 0.01\n",
    "#save_model(new_paras, path_f_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved\n"
     ]
    }
   ],
   "source": [
    "save_model(new_paras, path_f_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
