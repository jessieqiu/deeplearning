{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  4    5    6    7    8    9   10 ...  391  392  393  394  395  396  \\\n",
       "0                                           ...                                 \n",
       "0  0  0  0  0  0.0  0.0  0.0  0.0  0.0  0.0 ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0  0  0  0  0  0.0  0.0  0.0  0.0  0.0  0.0 ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0  0  0  0  0  0.0  0.0  0.0  0.0  0.0  0.0 ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0  0  0  0  0  0.0  0.0  0.0  0.0  0.0  0.0 ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0  0  0  0  0  0.0  0.0  0.0  0.0  0.0  0.0 ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   397  398  399  y  \n",
       "0                    \n",
       "0  0.0  0.0    0  9  \n",
       "0  0.0  0.0    0  6  \n",
       "0  0.0  0.0    0  5  \n",
       "0  0.0  0.0    0  7  \n",
       "0  0.0  0.0    0  1  \n",
       "\n",
       "[5 rows x 400 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ex3_train.csv\", index_col=0) \n",
    "df_test = pd.read_csv(\"ex3_test.csv\", index_col=0) \n",
    "df.head(5)\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(y):\n",
    "    if(y.shape[1]!=1):\n",
    "        print('input is wrong');\n",
    "        return -1\n",
    "    length = y.shape[0]\n",
    "    output = np.zeros((10,length))   \n",
    "    \n",
    "    for i in range(length):         \n",
    "        if(y[i][0]>9):\n",
    "            #to prevent index out of bound\n",
    "            print('label is wrong');\n",
    "            return -1\n",
    "        else:\n",
    "            output[y[i][0]][i]=1   \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dat shape (3500, 400) (3500, 1) (10, 3500)\n",
      "last five numbers:\n",
      "[[ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "test data shape (1500, 400) (1500, 1)\n"
     ]
    }
   ],
   "source": [
    "#split data\n",
    "size = df.shape[0]\n",
    "x = df.iloc[:,0:400]\n",
    "X = x.values\n",
    "y = df.iloc[:,-1]\n",
    "y_digit = y.values.reshape(size,1)\n",
    "y_one_hot = one_hot_encoding(y_digit)\n",
    "print (\"training dat shape\", X.shape, y_digit.shape, y_one_hot.shape)\n",
    "print('last five numbers:')\n",
    "print(y_one_hot[:, 3495:])\n",
    "\n",
    "size_test = df_test.shape[0]\n",
    "x_test = df_test.iloc[:,0:400]\n",
    "X_test = x_test.values\n",
    "y_test = df_test.iloc[:,-1].values.reshape(size_test,1)\n",
    "print (\"test data shape\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation_sigmoid(z): \n",
    "   \n",
    "    #print(z.shape)\n",
    "    a =  1.0/(1.0 + np.exp(-z))\n",
    "    #print(a.shape)\n",
    "    #reshape a here?\n",
    "    \n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16c8dd298d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test and plots for sigmoid functions\n",
    "z1 = np.arange(-5,5,0.1)\n",
    "a1 = activation_sigmoid(z1)\n",
    "plt.plot(z1, a1)\n",
    "plt.title('sigmoid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soft_max(f):\n",
    "    shift = f - np.max(f) #shift all number to the left so that the sum will not get too big or explode\n",
    "    exp_f = np.exp(f) \n",
    "    return exp_f / np.sum(exp_f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_one_layer (weights, X, bias): \n",
    "    z = np.dot(weights, X) + bias #bias is a one-column vector   \n",
    "    a = activation_sigmoid(z) \n",
    "    return z,a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_all_layers(X_data,weights_1, bias_1, weights_2,bias_2): \n",
    "    #hidden layer\n",
    "    z1,a1 = forward_one_layer(weights_1,X_data,bias_1)\n",
    "    \n",
    "    #output layer\n",
    "    z2,a2 = forward_one_layer(weights_2,a1,bias_2)        \n",
    "        \n",
    "    return z1,a1,z2,a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getGradient(a1, z2, a2, previous_dldz, w):\n",
    "    dzda = w \n",
    "    dadz = getActivationFunctionDerivative(a2)\n",
    "    dzdw = np.transpose(a1) #do i need transpose?\n",
    "    \n",
    "    #dzdw.shape (3500L, 30L)\n",
    "    #dzdb = 1    \n",
    "   \n",
    "    m = 3500    # n of samples\n",
    "    \n",
    "    #print('1')\n",
    "    #print( previous_dldz.shape,dzda.shape,dadz.shape)\n",
    "    # ((10L, 3500L), (10L, 20L), (20L, 3500L))\n",
    "    dldz = np.dot(previous_dldz.T,dzda).T * dadz\n",
    " \n",
    "    #print('2')\n",
    "    #print(dldz.shape,dzdw.shape)\n",
    "    \n",
    "    dldw = np.dot( dldz, dzdw )/m    \n",
    "    dldb = np.sum(dldz, axis = 1, keepdims = True)/(m) #  because dldb is dldz times dzdb which is 1 \n",
    "    #print('3')\n",
    "    #print(dldz.shape,dldw.shape,dldb.shape)\n",
    "    \n",
    "    return dldz, dldw, dldb \n",
    "    \n",
    "def getActivationFunctionDerivative(a):\n",
    "    \n",
    "    dadz = a * (1 - a)\n",
    "   \n",
    "    return dadz\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1  1  2]\n",
      " [ 1  2 -3]]\n",
      "[[ -2   0  -2]\n",
      " [  0  -2 -12]]\n"
     ]
    }
   ],
   "source": [
    "##test getActivationFunctionDerivative\n",
    "\n",
    "a = np.array([[-1,1,2],[1,2,-3]])\n",
    "print( a)\n",
    "\n",
    "dadz = getActivationFunctionDerivative(a)\n",
    "print(dadz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lastLayerGradient(a, X, i=0, j=y_one_hot.shape[1]):  #i j can be used for mini batch, X is the input to last layer\n",
    "    if(a.shape==y_one_hot[:,i:j].shape):\n",
    "        dldz = a - y_one_hot[:,i:j]\n",
    "        m = j-i\n",
    "        dldw = np.dot(dldz,np.transpose(X))/(m) # m or -m?\n",
    "        dldb = np.sum(dldz, axis = 1, keepdims = True)/(m)\n",
    "        #print(dldw.shape, dldb.shape)\n",
    "        #print('last layer done')\n",
    "        return dldz, dldw, dldb\n",
    "    else:\n",
    "        print('I did something wrong, shape of a is ', a.shape, ' it should be ', y_one_hot[:,i:j].shape)\n",
    "        return -1\n",
    "    \n",
    "def calculate_cross_entropy_loss(a, i=0, j=y_one_hot.shape[1]):\n",
    "    if(a.shape==y_one_hot[:,i:j].shape):        \n",
    "        if(a.any()==0):\n",
    "            a[a==0] = 0.00000001\n",
    "            #print ('shift to right')\n",
    "        if(a.any()==1):\n",
    "            a[a==1] = 0.99999999\n",
    "            #print ('shift to left')        \n",
    "            \n",
    "        temp1 = y_one_hot[:,i:j] * np.log(a)\n",
    "        temp2 = (1 - y_one_hot[:,i:j]) * np.log(1 - a)\n",
    "       \n",
    "        loss = (-1 / (j-i)) * np.sum(temp1 + temp2) #j must > i         \n",
    "    else:\n",
    "        print('I did something wrong again, shape of a is ', a.shape, ' it should be ', y_one_hot[:,i:j].shape)\n",
    "        loss = -1\n",
    "    return loss\n",
    "\n",
    "def accuracy(prediction,label):\n",
    "    if (prediction.shape!=label.shape):\n",
    "        print (prediction.shape)\n",
    "        print (label.shape)\n",
    "        print ('wrong input size')\n",
    "        return -1\n",
    "    count = 0\n",
    "    total = prediction.shape[0]\n",
    "    #print 'size ',total\n",
    "    for i in range(total):        \n",
    "        if prediction[i]!= label[i]:\n",
    "            count+=1\n",
    "            #print 'i', i\n",
    "    print ('error count:', count)\n",
    "    result = 0.0\n",
    "    error_rate = float(count)/total\n",
    "    accuracy = 1-error_rate\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialization(neurons):\n",
    "    \n",
    "    c= [1.0, 1.0]\n",
    "    for i in range(len(neurons)):    \n",
    "        \n",
    "        c[i] = math.sqrt(2)\n",
    "        \n",
    "#         c[i] = 4 * math.sqrt(2)\n",
    "#         c[i] = 2    \n",
    "    print ('sigma,', c)   \n",
    "    np.random.seed(0)\n",
    "    weights_1 = np.random.normal(0, c[0] / math.sqrt(neurons[0] + 400) ,(neurons[0],400))\n",
    "    weights_2 = np.random.normal(0, c[1] / math.sqrt(neurons[1] + neurons[0]), (neurons[1],neurons[0]))\n",
    "    print (\"shape of weights: \", weights_1.shape, weights_2.shape )\n",
    "\n",
    "\n",
    "#     bias_1 = np.random.normal(0, c[0] / math.sqrt(neurons[0]), (neurons[0],1))  \n",
    "#     bias_2 = np.random.normal(0, c[1] / math.sqrt(neurons[1]), (neurons[1],1))\n",
    "    \n",
    "    bias_1 = np.zeros((neurons[0],1))  \n",
    "    bias_2 = np.zeros((neurons[1],1))\n",
    "    \n",
    "    print (\"shape of bias: \", bias_1.shape, bias_2.shape  )   \n",
    "\n",
    "    return weights_1, bias_1, weights_2,bias_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma, [1.4142135623730951, 1.4142135623730951]\n",
      "shape of weights:  (25, 400) (10, 25)\n",
      "shape of bias:  (25, 1) (10, 1)\n"
     ]
    }
   ],
   "source": [
    "#initialization  #no need to run all the time, can skip and continue training\n",
    "neurons = [25,10]\n",
    "weights_1, bias_1, weights_2,bias_2 = initialization(neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma, [1.4142135623730951, 1.4142135623730951]\n",
      "shape of weights:  (25, 400) (10, 25)\n",
      "shape of bias:  (25, 1) (10, 1)\n",
      "iteration:  0 loss:  7.86892515793\n",
      "iteration:  100 loss:  2.63911900987\n",
      "error count: 389\n",
      "New best accuracy:  0.7406666666666666 at i= 100 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  200 loss:  2.1337545736\n",
      "error count: 207\n",
      "New best accuracy:  0.862 at i= 200 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  300 loss:  1.74225766086\n",
      "error count: 171\n",
      "New best accuracy:  0.886 at i= 300 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  400 loss:  1.45351804911\n",
      "error count: 160\n",
      "New best accuracy:  0.8933333333333333 at i= 400 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  500 loss:  1.24753127525\n",
      "error count: 146\n",
      "New best accuracy:  0.9026666666666667 at i= 500 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  600 loss:  1.09591008244\n",
      "error count: 134\n",
      "New best accuracy:  0.9106666666666666 at i= 600 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  700 loss:  0.979318490672\n",
      "error count: 123\n",
      "New best accuracy:  0.918 at i= 700 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  800 loss:  0.886563867127\n",
      "error count: 116\n",
      "New best accuracy:  0.9226666666666666 at i= 800 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  900 loss:  0.811098227694\n",
      "error count: 115\n",
      "New best accuracy:  0.9233333333333333 at i= 900 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  1000 loss:  0.748744398457\n",
      "error count: 105\n",
      "New best accuracy:  0.9299999999999999 at i= 1000 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  1100 loss:  0.696576822781\n",
      "error count: 106\n",
      "iteration:  1200 loss:  0.652424050601\n",
      "error count: 101\n",
      "New best accuracy:  0.9326666666666666 at i= 1200 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  1300 loss:  0.614632288686\n",
      "error count: 101\n",
      "iteration:  1400 loss:  0.581929739466\n",
      "error count: 100\n",
      "New best accuracy:  0.9333333333333333 at i= 1400 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  1500 loss:  0.553334142846\n",
      "error count: 96\n",
      "New best accuracy:  0.9359999999999999 at i= 1500 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  1600 loss:  0.528084023217\n",
      "error count: 93\n",
      "New best accuracy:  0.938 at i= 1600 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  1700 loss:  0.505586222961\n",
      "error count: 92\n",
      "New best accuracy:  0.9386666666666666 at i= 1700 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  1800 loss:  0.485375837465\n",
      "error count: 89\n",
      "New best accuracy:  0.9406666666666667 at i= 1800 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  1900 loss:  0.467085792196\n",
      "error count: 89\n",
      "iteration:  2000 loss:  0.450423856728\n",
      "error count: 89\n",
      "iteration:  2100 loss:  0.435155321669\n",
      "error count: 87\n",
      "New best accuracy:  0.942 at i= 2100 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  2200 loss:  0.421089943218\n",
      "error count: 84\n",
      "New best accuracy:  0.944 at i= 2200 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  2300 loss:  0.408072083119\n",
      "error count: 82\n",
      "New best accuracy:  0.9453333333333334 at i= 2300 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  2400 loss:  0.395973234417\n",
      "error count: 82\n",
      "iteration:  2500 loss:  0.38468632858\n",
      "error count: 81\n",
      "New best accuracy:  0.946 at i= 2500 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  2600 loss:  0.374121375614\n",
      "error count: 80\n",
      "New best accuracy:  0.9466666666666667 at i= 2600 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  2700 loss:  0.364202105236\n",
      "error count: 80\n",
      "iteration:  2800 loss:  0.354863363124\n",
      "error count: 80\n",
      "iteration:  2900 loss:  0.346049079384\n",
      "error count: 78\n",
      "New best accuracy:  0.948 at i= 2900 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  3000 loss:  0.337710672655\n",
      "error count: 77\n",
      "New best accuracy:  0.9486666666666667 at i= 3000 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  3100 loss:  0.329805787343\n",
      "error count: 75\n",
      "New best accuracy:  0.95 at i= 3100 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  3200 loss:  0.32229728662\n",
      "error count: 74\n",
      "New best accuracy:  0.9506666666666667 at i= 3200 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  3300 loss:  0.315152442599\n",
      "error count: 74\n",
      "iteration:  3400 loss:  0.30834227919\n",
      "error count: 74\n",
      "iteration:  3500 loss:  0.301841033819\n",
      "error count: 74\n",
      "iteration:  3600 loss:  0.2956257123\n",
      "error count: 73\n",
      "New best accuracy:  0.9513333333333334 at i= 3600 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  3700 loss:  0.289675717361\n",
      "error count: 73\n",
      "iteration:  3800 loss:  0.283972535968\n",
      "error count: 72\n",
      "New best accuracy:  0.952 at i= 3800 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  3900 loss:  0.278499474164\n",
      "error count: 71\n",
      "New best accuracy:  0.9526666666666667 at i= 3900 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  4000 loss:  0.273241430765\n",
      "error count: 71\n",
      "iteration:  4100 loss:  0.268184703222\n",
      "error count: 71\n",
      "iteration:  4200 loss:  0.263316820433\n",
      "error count: 70\n",
      "New best accuracy:  0.9533333333333334 at i= 4200 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  4300 loss:  0.258626398358\n",
      "error count: 70\n",
      "iteration:  4400 loss:  0.254103015085\n",
      "error count: 70\n",
      "iteration:  4500 loss:  0.249737102616\n",
      "error count: 71\n",
      "iteration:  4600 loss:  0.245519853051\n",
      "error count: 70\n",
      "iteration:  4700 loss:  0.241443137241\n",
      "error count: 70\n",
      "iteration:  4800 loss:  0.237499434203\n",
      "error count: 69\n",
      "New best accuracy:  0.954 at i= 4800 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  4900 loss:  0.233681769845\n",
      "error count: 68\n",
      "New best accuracy:  0.9546666666666667 at i= 4900 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  5000 loss:  0.229983663708\n",
      "error count: 68\n",
      "iteration:  5100 loss:  0.226399082605\n",
      "error count: 68\n",
      "iteration:  5200 loss:  0.222922400157\n",
      "error count: 68\n",
      "iteration:  5300 loss:  0.219548361379\n",
      "error count: 68\n",
      "iteration:  5400 loss:  0.216272051547\n",
      "error count: 68\n",
      "iteration:  5500 loss:  0.213088868708\n",
      "error count: 67\n",
      "New best accuracy:  0.9553333333333334 at i= 5500 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  5600 loss:  0.209994499275\n",
      "error count: 67\n",
      "iteration:  5700 loss:  0.206984896223\n",
      "error count: 67\n",
      "iteration:  5800 loss:  0.204056259494\n",
      "error count: 67\n",
      "iteration:  5900 loss:  0.201205018252\n",
      "error count: 67\n",
      "iteration:  6000 loss:  0.198427814731\n",
      "error count: 66\n",
      "New best accuracy:  0.956 at i= 6000 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  6100 loss:  0.195721489415\n",
      "error count: 65\n",
      "New best accuracy:  0.9566666666666667 at i= 6100 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  6200 loss:  0.193083067371\n",
      "error count: 65\n",
      "iteration:  6300 loss:  0.190509745573\n",
      "error count: 65\n",
      "iteration:  6400 loss:  0.187998881104\n",
      "error count: 65\n",
      "iteration:  6500 loss:  0.185547980132\n",
      "error count: 65\n",
      "iteration:  6600 loss:  0.183154687597\n",
      "error count: 65\n",
      "iteration:  6700 loss:  0.180816777551\n",
      "error count: 64\n",
      "New best accuracy:  0.9573333333333334 at i= 6700 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  6800 loss:  0.178532144137\n",
      "error count: 64\n",
      "iteration:  6900 loss:  0.17629879315\n",
      "error count: 64\n",
      "iteration:  7000 loss:  0.174114834204\n",
      "error count: 64\n",
      "iteration:  7100 loss:  0.171978473462\n",
      "error count: 64\n",
      "iteration:  7200 loss:  0.169888006936\n",
      "error count: 64\n",
      "iteration:  7300 loss:  0.167841814339\n",
      "error count: 63\n",
      "New best accuracy:  0.958 at i= 7300 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  7400 loss:  0.165838353475\n",
      "error count: 62\n",
      "New best accuracy:  0.9586666666666667 at i= 7400 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  7500 loss:  0.163876155152\n",
      "error count: 62\n",
      "iteration:  7600 loss:  0.161953818582\n",
      "error count: 62\n",
      "iteration:  7700 loss:  0.160070007274\n",
      "error count: 62\n",
      "iteration:  7800 loss:  0.15822344535\n",
      "error count: 62\n",
      "iteration:  7900 loss:  0.156412914299\n",
      "error count: 62\n",
      "iteration:  8000 loss:  0.154637250104\n",
      "error count: 62\n",
      "iteration:  8100 loss:  0.152895340733\n",
      "error count: 62\n",
      "iteration:  8200 loss:  0.151186123957\n",
      "error count: 62\n",
      "iteration:  8300 loss:  0.149508585448\n",
      "error count: 62\n",
      "iteration:  8400 loss:  0.147861757147\n",
      "error count: 62\n",
      "iteration:  8500 loss:  0.14624471583\n",
      "error count: 61\n",
      "New best accuracy:  0.9593333333333334 at i= 8500 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  8600 loss:  0.144656581843\n",
      "error count: 60\n",
      "New best accuracy:  0.96 at i= 8600 with neurons [25, 10] at learning rate 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  8700 loss:  0.143096517949\n",
      "error count: 60\n",
      "iteration:  8800 loss:  0.141563728212\n",
      "error count: 60\n",
      "iteration:  8900 loss:  0.140057456868\n",
      "error count: 60\n",
      "iteration:  9000 loss:  0.138576987095\n",
      "error count: 60\n",
      "iteration:  9100 loss:  0.137121639621\n",
      "error count: 59\n",
      "New best accuracy:  0.9606666666666667 at i= 9100 with neurons [25, 10] at learning rate 0.1\n",
      "iteration:  9200 loss:  0.135690771115\n",
      "error count: 59\n",
      "iteration:  9300 loss:  0.134283772308\n",
      "error count: 59\n",
      "iteration:  9400 loss:  0.132900065836\n",
      "error count: 59\n",
      "iteration:  9500 loss:  0.131539103816\n",
      "error count: 59\n",
      "iteration:  9600 loss:  0.130200365209\n",
      "error count: 59\n",
      "iteration:  9700 loss:  0.128883353031\n",
      "error count: 59\n",
      "iteration:  9800 loss:  0.127587591542\n",
      "error count: 59\n",
      "iteration:  9900 loss:  0.126312623495\n",
      "error count: 59\n",
      "sigma, [1.4142135623730951, 1.4142135623730951]\n",
      "shape of weights:  (25, 400) (10, 25)\n",
      "shape of bias:  (25, 1) (10, 1)\n",
      "iteration:  0 loss:  7.86892515793\n",
      "iteration:  100 loss:  2.13536699665\n",
      "error count: 207\n",
      "iteration:  200 loss:  1.45403679113\n",
      "error count: 160\n",
      "iteration:  300 loss:  1.09680196094\n",
      "error count: 134\n",
      "iteration:  400 loss:  0.887891357832\n",
      "error count: 116\n",
      "iteration:  500 loss:  0.750259126489\n",
      "error count: 106\n",
      "iteration:  600 loss:  0.65393998028\n",
      "error count: 103\n",
      "iteration:  700 loss:  0.583408588971\n",
      "error count: 99\n",
      "iteration:  800 loss:  0.529534458541\n",
      "error count: 96\n",
      "iteration:  900 loss:  0.486807139105\n",
      "error count: 90\n",
      "iteration:  1000 loss:  0.451836590133\n",
      "error count: 89\n",
      "iteration:  1100 loss:  0.422477207923\n",
      "error count: 87\n",
      "iteration:  1200 loss:  0.397323988894\n",
      "error count: 82\n",
      "iteration:  1300 loss:  0.37542365838\n",
      "error count: 81\n",
      "iteration:  1400 loss:  0.356106740318\n",
      "error count: 80\n",
      "iteration:  1500 loss:  0.3388876465\n",
      "error count: 79\n",
      "iteration:  1600 loss:  0.323403696955\n",
      "error count: 74\n",
      "iteration:  1700 loss:  0.309376973801\n",
      "error count: 74\n",
      "iteration:  1800 loss:  0.296589848119\n",
      "error count: 74\n",
      "iteration:  1900 loss:  0.284868810405\n",
      "error count: 74\n",
      "iteration:  2000 loss:  0.27407341738\n",
      "error count: 72\n",
      "iteration:  2100 loss:  0.264088471267\n",
      "error count: 71\n",
      "iteration:  2200 loss:  0.254818323906\n",
      "error count: 70\n",
      "iteration:  2300 loss:  0.246182646446\n",
      "error count: 69\n",
      "iteration:  2400 loss:  0.238113254707\n",
      "error count: 67\n",
      "iteration:  2500 loss:  0.230551715937\n",
      "error count: 68\n",
      "iteration:  2600 loss:  0.223447538816\n",
      "error count: 68\n",
      "iteration:  2700 loss:  0.21675679623\n",
      "error count: 67\n",
      "iteration:  2800 loss:  0.21044106525\n",
      "error count: 67\n",
      "iteration:  2900 loss:  0.204466596741\n",
      "error count: 66\n",
      "iteration:  3000 loss:  0.19880364981\n",
      "error count: 66\n",
      "iteration:  3100 loss:  0.193425944483\n",
      "error count: 65\n",
      "iteration:  3200 loss:  0.18831019999\n",
      "error count: 65\n",
      "iteration:  3300 loss:  0.183435736885\n",
      "error count: 65\n",
      "iteration:  3400 loss:  0.178784129419\n",
      "error count: 64\n",
      "iteration:  3500 loss:  0.174338900639\n",
      "error count: 64\n",
      "iteration:  3600 loss:  0.170085256598\n",
      "error count: 64\n",
      "iteration:  3700 loss:  0.166009857929\n",
      "error count: 64\n",
      "iteration:  3800 loss:  0.162100627343\n",
      "error count: 63\n",
      "iteration:  3900 loss:  0.158346590911\n",
      "error count: 63\n",
      "iteration:  4000 loss:  0.154737750024\n",
      "error count: 63\n",
      "iteration:  4100 loss:  0.151264980266\n",
      "error count: 63\n",
      "iteration:  4200 loss:  0.14791995298\n",
      "error count: 61\n",
      "iteration:  4300 loss:  0.144695075054\n",
      "error count: 60\n",
      "iteration:  4400 loss:  0.141583441789\n",
      "error count: 60\n",
      "iteration:  4500 loss:  0.138578796769\n",
      "error count: 60\n",
      "iteration:  4600 loss:  0.135675491572\n",
      "error count: 60\n",
      "iteration:  4700 loss:  0.132868438086\n",
      "error count: 60\n",
      "iteration:  4800 loss:  0.130153048361\n",
      "error count: 60\n",
      "iteration:  4900 loss:  0.12752516172\n",
      "error count: 60\n",
      "iteration:  5000 loss:  0.124980964942\n",
      "error count: 60\n",
      "iteration:  5100 loss:  0.122516915741\n",
      "error count: 59\n",
      "iteration:  5200 loss:  0.12012967985\n",
      "error count: 59\n",
      "iteration:  5300 loss:  0.117816087717\n",
      "error count: 57\n",
      "New best accuracy:  0.962 at i= 5300 with neurons [25, 10] at learning rate 0.2\n",
      "iteration:  5400 loss:  0.11557311072\n",
      "error count: 57\n",
      "iteration:  5500 loss:  0.113397852132\n",
      "error count: 57\n",
      "iteration:  5600 loss:  0.111287546345\n",
      "error count: 56\n",
      "New best accuracy:  0.9626666666666667 at i= 5600 with neurons [25, 10] at learning rate 0.2\n",
      "iteration:  5700 loss:  0.109239560733\n",
      "error count: 56\n",
      "iteration:  5800 loss:  0.107251396609\n",
      "error count: 56\n",
      "iteration:  5900 loss:  0.105320687703\n",
      "error count: 56\n",
      "iteration:  6000 loss:  0.103445196033\n",
      "error count: 56\n",
      "iteration:  6100 loss:  0.101622805684\n",
      "error count: 56\n",
      "iteration:  6200 loss:  0.0998515153092\n",
      "error count: 56\n",
      "iteration:  6300 loss:  0.0981294300273\n",
      "error count: 56\n",
      "iteration:  6400 loss:  0.0964547532862\n",
      "error count: 54\n",
      "New best accuracy:  0.964 at i= 6400 with neurons [25, 10] at learning rate 0.2\n",
      "iteration:  6500 loss:  0.0948257790395\n",
      "error count: 54\n",
      "iteration:  6600 loss:  0.0932408844399\n",
      "error count: 54\n",
      "iteration:  6700 loss:  0.0916985231429\n",
      "error count: 55\n",
      "iteration:  6800 loss:  0.0901972192367\n",
      "error count: 55\n",
      "iteration:  6900 loss:  0.0887355617825\n",
      "error count: 54\n",
      "iteration:  7000 loss:  0.0873121999315\n",
      "error count: 54\n",
      "iteration:  7100 loss:  0.0859258385794\n",
      "error count: 54\n",
      "iteration:  7200 loss:  0.0845752345251\n",
      "error count: 54\n",
      "iteration:  7300 loss:  0.0832591930975\n",
      "error count: 54\n",
      "iteration:  7400 loss:  0.0819765652141\n",
      "error count: 54\n",
      "iteration:  7500 loss:  0.0807262448347\n",
      "error count: 54\n",
      "iteration:  7600 loss:  0.079507166765\n",
      "error count: 54\n",
      "iteration:  7700 loss:  0.0783183047627\n",
      "error count: 54\n",
      "iteration:  7800 loss:  0.0771586698975\n",
      "error count: 54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-54140f857a43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# X is training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mz1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_all_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbias_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-19d47717836f>\u001b[0m in \u001b[0;36mforward_all_layers\u001b[1;34m(X_data, weights_1, bias_1, weights_2, bias_2)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward_all_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbias_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mz1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_one_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbias_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#output layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-ba614345d4b7>\u001b[0m in \u001b[0;36mforward_one_layer\u001b[1;34m(weights, X, bias)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward_one_layer\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;31m#bias is a one-column vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivation_sigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#main function\n",
    "max_iterations = 10000\n",
    "learning_rates = [0.1, 0.2, 0.5]\n",
    "best_test_accuracy = 0.0\n",
    "plot_x = np.arange(0,max_iterations,100)\n",
    "plot_y = np.zeros((len(learning_rates),100))\n",
    "\n",
    "for j in range(len(learning_rates)):\n",
    "    weights_1, bias_1, weights_2,bias_2 = initialization(neurons)\n",
    "    previous_loss = 99999999\n",
    "    previous_accuracy = 0.0\n",
    "    for i in range(max_iterations):    \n",
    "\n",
    "        # X is training data\n",
    "        z1,a1,z2,a2 = forward_all_layers(np.transpose(X),weights_1, bias_1, weights_2,bias_2)\n",
    "\n",
    "        if(i%100==0):\n",
    "           \n",
    "            loss = calculate_cross_entropy_loss(a2)\n",
    "           \n",
    "            te = int(i/100)\n",
    "            plot_y[j][te] = loss\n",
    "            print ('iteration: ', i, \"loss: \", loss)\n",
    "            \n",
    "            if(loss - previous_loss > 10000 or abs(previous_loss - loss) < 0.00000001 or   math.isnan(loss)):\n",
    "                print('loss is getting worse')\n",
    "                break    \n",
    "            previous_loss = loss \n",
    "\n",
    "        #back propagation\n",
    "        dldz_2, dldw_2, dldb_2 = lastLayerGradient(a2,a1)   \n",
    "        dldz_1, dldw_1, dldb_1 = getGradient(X.T, z1, a1, dldz_2, weights_2)\n",
    "\n",
    "        #update weights    \n",
    "        weights_1 -= learning_rates[j]*dldw_1\n",
    "        weights_2 -= learning_rates[j]*dldw_2\n",
    "\n",
    "        #update bias \n",
    "        bias_1 -= learning_rates[j]*dldb_1\n",
    "        bias_2 -= learning_rates[j]*dldb_2\n",
    "\n",
    "        if(i > 0 and i % 100 == 0  ):        \n",
    "            _,_,_,a3_test = forward_all_layers(np.transpose(X_test),weights_1, bias_1, weights_2,bias_2)\n",
    "            predict_test = np.argmax(a3_test, axis=0)\n",
    "            test_length = len(predict_test)\n",
    "            test_acc = accuracy(predict_test.reshape(test_length,1),y_test)\n",
    "            if (test_acc > best_test_accuracy):\n",
    "                best_test_accuracy = test_acc\n",
    "                print ('New best accuracy: ', test_acc, 'at i=', i, 'with neurons', neurons, 'at learning rate', learning_rates[j])\n",
    "                best1, best2, best3, best4 = weights_1, bias_1, weights_2,bias_2\n",
    "                #early stopping\n",
    "#             if (test_acc < previous_accuracy - 0.01 and test_acc > 0.92  or test_acc < best_test_accuracy - 0.01):\n",
    "#                 print('Break. Test data accuracy starts to decrease, ', test_acc, 'at iteration', i)\n",
    "#                 break\n",
    "            previous_accuracy = test_acc\n",
    "\n",
    "print (\"Total number of iterations \", i+1)\n",
    "print ('My optimized test data accuracy', best_test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multiple line plot\n",
    "plt.plot( plot_x, plot_y[0], marker='', color='skyblue', linewidth=2, label=\"learning rate = 0.1\")\n",
    "plt.plot( plot_x, plot_y[1], marker='', color='olive', linewidth=2, label=\"learning rate = 0.2\")\n",
    "plt.plot( plot_x, plot_y[2], marker='', color='red', linewidth=2, label=\"learning rate = 0.5\")\n",
    "plt.legend()\n",
    "plt.title('cost vs number of iterations')\n",
    "plt.ylim(0,5000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_,_,_,a3 = forward_all_layers(np.transpose(X),best1, best2,best3,best4)\n",
    "predict_train_final = np.argmax(soft_max(a3), axis=0)\n",
    "L = len(predict_train_final)\n",
    "\n",
    "print 'final train data accuracy. ',accuracy(predict_train_final.reshape(L,1),y_digit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_,_,_,a3_final = forward_all_layers(np.transpose(X_test),best1, best2,best3,best4)\n",
    "predict_final = np.argmax(soft_max(a3_final), axis=0)\n",
    "L = len(predict_final)\n",
    "print 'final test data accuracy. ', accuracy(predict_final.reshape(L,1),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
